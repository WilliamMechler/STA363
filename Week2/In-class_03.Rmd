---
title: "Class 03: Paired *t*-test; Intro to Design of Experiments"
author: "Hughes/Fisher"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Packages we will use today; note one new pacakge: `ggfortify`.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(ggfortify)
```

# Example: A paired $t$-test

**Let's revisit the MKT instructor evaluation dataset**, but ask a different question that involves looking at *paired data* rather than independent samples (which we saw in Class 02).  You may look back at the Class 02 RMarkdown to remind yourself of the complete contents of the dataset, but here is are the relevant variables for this upcoming question:

* `Instructor`	-- Random instructor ID
* `Course`	-- Course Type (1=FSB Core, 2=MKT Core, 3=MKT Elective, 4=Marketing Capstone)
* `iStandards` -- *"The instructor held students to high academic standards."* (Scored on 0-4 scale)  
* `iEnthusiasm`	-- *"The instructor showed enthusiasm for the subject."*  (Scored on 0-4 scale) 
 
**Research question:** Among FSB Core courses, is there significant evidence to conclude that students (on average) rate instructors higher on enthusiasm as opposed to holding students to high academic standards?

The standard protocol for analysis is:

1. Read in the data
2. Effectively clean and filter the data we need
3. Perform the EDA and make observations
4. Check assumptions underlying the formal analysis
5. Perform the inferential analysis (hypothesis test/CI) and interpret the result

```{r readData}
evals <- read.csv("http://users.miamioh.edu/hughesmr/sta363/teachingEvals.csv")
```

```{r cleanData, message=FALSE}
# Filter FSB Core courses only, retain relevant variables
FSBcoreevals <- evals %>%
  mutate(Instructor=as.factor(Instructor)) %>%
  filter(Course == 1) %>%
  group_by(Instructor) %>%
  summarize(iStandards = mean(iStandards),
            iEnthusiasm = mean(iEnthusiasm)) %>%
  select(Instructor, iStandards, iEnthusiasm)

head(FSBcoreevals)
```

A plot illustrating the distributions of evalations scores in these two areas, as well as stressing the paired nature of the evaluations by instructor, is given below.  The solid red lines connect the sample means between the two items of evaluation; the transparent lines link the mean evaluations by each individual instructor:

```{r plotData, echo=FALSE}
# Process data and convert to tall mode to create profile plot
FSBcoreevals.tall <- FSBcoreevals %>%
  pivot_longer(c(iStandards, iEnthusiasm), names_to="EvalType", values_to="Mean") %>%
  mutate(EvalType=factor(EvalType, levels=c("iStandards","iEnthusiasm")))

# Make profile plot with mean overlays
ggplot(FSBcoreevals.tall, aes(x = EvalType, y = Mean)) + 
  geom_boxplot() +
  geom_point(aes(group = Instructor, color = Instructor), alpha = 0.4) +
  geom_line(aes(group = Instructor, color = Instructor), alpha = 0.4) + 
  stat_summary(fun = "mean", size = 3, geom = "point", color="red") +
  stat_summary(fun = "mean", size = 1, geom = "line", color="red", aes(group = 1)) +
  theme(legend.position = "none") +
  ylab("Mean Evaluation") +
  xlab("Evaluation Item") 
```

There is some indication that instructors in FSB Core courses are generally rated higher on enthusiasm than on holding students to high academic standards; however, there is a lot of instructor-to-instructor variabiity.  

Before doing the paired *t*-test, let's check the assumption of normality of the difference scores.  We begin by calculating each instructor's mean difference score, and then building a normal Q-Q plot for these differences:

```{r}
FSBcoreevals <- FSBcoreevals %>%
  mutate(Difference = iStandards - iEnthusiasm)

# Check the pairwise calculation
head(FSBcoreevals)

# Make the normal Q-Q plot
ggplot(FSBcoreevals) + 
  geom_qq(aes(sample = Difference)) + 
  geom_qq_line(aes(sample = Difference))
```

Normality looks fine here, so we proceed with the formal paired *t*-test:

```{r}
t.test(FSBcoreevals$iStandards, FSBcoreevals$iEnthusiasm, paired=TRUE)
```

There is a significant difference in the true mean instructor rating on enthusiasm vs. holding students to high academic standards in FSB Core courses (*t* = -2.775, *p*-value = 0.01204).  We can be 95% confident that the true mean instructor rating on holding students to high academic standards is between 0.0265 to 0.1894 points lower than the corresponding mean rating on enthusiasm.

See the textbook for more details on implementing a paired $t$-test in R.


# Designed Experiments

In many situations, a practitioner simply collects measurements on predictor and response variables as they naturally occur, without intervention from the data collector. Such data is called **observational data** or data derived from an **observational study**.  An example is the MKT Teaching Evaluations data.

In a **designed experiment**, a researcher *manipulates* one or more variables, while holding all other variables constant. Typically the values of the predictor variables are discrete (that is, a countably finite number of controlled values). The main advantage of well-designed experiments over observational studies is that we can establish cause and effect relationships between the predictors and response. One of the most important things to keep in mind with analyzing designed experiments is that **the structure of the experiment dictates how the analysis may proceed**. 

### Example: A paired-data experimental design

Twenty (20) mice received a dietary treatment during 3 months. We want to know whether the dietary treatment has an impact on the weight of the mice. To answer to this question, the weight of the 20 mice have been measured before and after the treatment.

* **Response variable**: change of mice weight (typically after - before)
* **Experimental Unit**: A mouse
* **Factor**: Here, the factor is **Time**, because the researcher chose the time (after = 3 months after before). Diet is NOT the factor here, because it is not a variable in this study: every mouse gets the same dietary treatment.
* **Controls**: Pairing - controls for nuisance variation within each mouse.

*Note:* Even though the data collected here is from a **designed experiment** rather than an observational study, this would still be analyzed using a paired *t*-test. 

*Note #2:* Although we are using pairing in this experiment, it is far from a *perfectly* designed experiment. The pairing helps control nuisance variation that each mouse may contribute, but other nuisance effects may still be present (for example, environmental factors on a particular mouse).


# One-Way ANOVA (Analysis of Variance)

Let's return to the setting of independent (not paired) samples again (see Class 02).  When comparing two populations, we used an independent-samples *t*-test before. Consider the following:

**Example:** A tire manufacturer is interested in investigating the handling properties for different tread patterns. Data were recorded on the stopping distances measured to the nearest foot for a standard sized car to come to a complete stop from a speed of 60 miles per hour. There are six measurements of the stopping distance for each of four different tread patterns labeled A, B, C and D. The same driver and car were used for all 24 measurements and, although not clear from the saved data file, the order of treatments were assigned at random.

*Source*: Ugarte, M. D., Militino, A. F., and Arnholt, A. T. (2008) Probability and Statistics with R. Chapman \& Hall/CRC.

```{r}
Tire <- read.csv("http://users.miamioh.edu/hughesmr/sta363/TireData.csv")
glimpse(Tire)
```

Answer the following questions about the experimental design.

* What are the **Experimental Units** in this study? 
     - *The runs of the car are the EUs because they are the entities to which treatments are being applied.*
* What is the **factor** in this study? 
     - *Tread type*
* How many **factor levels** are there? 
     - *Four tread levels: A, B, C, and D** *(Note: ONE facor wth FOUR levels!)*
* What are the **treatments**?  
     - *(We will answer in class)*
* What other steps were taken to control for nuisance variables and unexplained variability? 
     - *(In class, discuss randomization, replication, same driver, same car -- how all these will control for some variability)*


### EDA

As with all analyses, we begin with an exploratory data analysis. This will reveal some basic information regarding the data. Since we are interested in tire-level information, we `group_by` tire type and calculuate summary values.

```{r, message=FALSE}
Tire %>% 
  group_by(tire) %>% 
  summarize(Mean=mean(StopDist), 
            SD=sd(StopDist),
            Var=var(StopDist), 
            N=n() ) %>% 
  kable()
```

Next, we wish to graphical explore the data. In the code chunk below, write code that will construct side-by-side boxplots of the stopping distance as a function of the tire type.

```{r}
ggplot(Tire, aes(x=tire, y=StopDist)) +
  geom_boxplot() +
  labs(x="Tire Tread Type", y="Stopping Distance") + 
  theme_bw()
```

Discuss the pros and cons of the above plot

* **Pro**: *Provides a nice visual way to compare the groups.*
* **Con**: *Kind of silly to build boxplots here, since we only have 6 observations per group. Why not just report the whole data?*

```{r}
ggplot(Tire, aes(x=tire, y=StopDist)) +
  geom_jitter(width=0.1) +
  labs(x="Tire Tread Type", y="Stopping Distance") + 
  theme_bw()
```

This is known as a *Jittered* dot plot - we have added some horizontal variation in the data strictly for visualizing the data (the data itself it not being changed).

* **Pro**: *Shows all the data, with only 6 observations this is okay.*
* **Con**: *With a much larger dataset, this plot could become cumbersome and less useful.*

## The Analysis - ANOVA

The key difference in this problem is that we are now **comparing more than two populations**, rather than just two like we did earlier.  To analyze this sort of data problem, we will instead perform a One-Way Analysis of Variance (ANOVA).  

### Ideas of ANOVA

We begin with some basic analysis that will also help us explain the concept of One-Way ANOVA. First consider the following overall summary values:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
kable(Tire %>% summarize(Mean=mean(StopDist), Var=var(StopDist), N=n(), SS=var(StopDist)*(n()-1)))
```

You will note we calculate an **SS** which corresponds to **sum of squares**. Mathematically it is

$$SS_{Total} = \sum_{i=1}^n (Y_i - \bar{Y})^2$$

for observations $Y_i, i=1,\ldots,n$. We call this the sum of squares *total* because it is the total sum of squares in the *entire* sample. You should note this equation is nothing more than $S^2$ from your Intro Statistics course, except it is missing the degrees of freedom $n-1$.

Ultimately, we are interested in comparing the four groups and we saw earlier it appears the type of tire may matter. So how does each tire treatment perform?

```{r, echo=FALSE, message=FALSE}
kable(Tire %>% group_by(tire) %>% summarize(Mean=mean(StopDist), Var=var(StopDist), N=n(), SS=var(StopDist)*(n()-1) ))
```

Ultimately we are interested in statistically testing if the type of tire (tread type) influences the mean stopping distance. Statistically we test the null hypothesis of **simultaneous equality of all four true mean stopping distances**:

$$H_0: \mu_A = \mu_B = \mu_C = \mu_D$$
versus 
$$H_A: \mu_i \neq \mu_j, ~\textrm{for some}~~ i,j=A,B,C,D$$

If the null hypothesis were true we would expect each of the four sample mean values of stopping distances to be approximately equal, and they should reasonably match the overall sample mean.

A way to measure the difference in the means compared to the overall mean is to look at the sum of squares for each group mean; mathematically:
$$(379.6667-404.2083)^2 + (405.1667-404.2083)^2 + (421.6667-404.2083)^2 + (410.3333-404.2083)^2 = 5673.125$$

If the null hypothesis were true, the above quantity should be reasonably close to the value of **zero**. If the value greatly exceeds zero, then we could argue that **at least one** of the tire tread's true mean stopping distance is different enough from the others, and we would reject the null hypothesis. The above value is typically known as the **sum of squares model**, **sum of squares treatments**, or the **between groups sum of squares**, labeled $SS_{Treatments}$.

Now the value of 5673.125 seems awfully far away from zero, but we have not accounted for any variability in that measurement (think back to one sample $t$-test, in the numerator you have $\bar{x}-\mu_0$ but that difference is scaled based on the standard error, $s/\sqrt{n}$). We need to account for variability. To consider that, first note that the $SS_{Treatments}$ is essentially a measure of variance. The $SS_{Treatments}$ essentially measures how much variability in the $SS_{Total}$ is explained by the different treatments (*between* the groups). What is left is *unexplained*, or is still within the treatments. This can is determined from the residuals *within* each group (a residual for a single point is $x_i - \bar{x}$), which essentially measures how much random error (or *noise*) is left after we modeled $x_i$ with $\bar{x}$. We can find the total amount of variability unexplained with:

$$SS_{Error} = \sum_{j=1}^{K}\sum_{i=1}^{n_k} (Y_{j,i}-\bar{Y}_{j})^2$$
for $K$ different groups each of size $n_k$. You'll note the inside summation is essentially the variance (lacking the degrees of freedom) of each group. We have that in a table above!
$$SS_{Error} = 2471.333 + 852.8333 + 747.3333 + 3027.3333 = 7098.833$$
Now, note the following:
$$5673.125 + 7098.833 = 12771.96$$
which corresponds to the sum of squares total!

### How does ANOVA work?

In the above example, we essentially **decomposed** the variance in the total sample (the sum of squares total) into two parts, the sum of squares *between* the groups (the model or treatments part) and the sum of squares *within* the groups (the error, or *residuals*). If the null hypothesis is true, we would expect $SS_{Treatment}\approx 0$, thus $SS_{Error}\approx SS_{Total}$. If the null hypothesis were not true, we would expect the different treatments to explain most of the variability and thus $SS_{Treatment} \approx SS_{Total}$ with $SS_{Error}\approx 0$.

This process is called Analysis of Variance (ANOVA) because we essentially are comparing variance estimates. The statistic we use is an $F$-statistic, which is based on the Sum of Squares but also incorporates the degrees of freedom to make a proper variance comparison: 

$$F = \frac{SS_{Treatment}/(K-1)}{SS_{Error}/(n_1+\ldots+n_k - K)} = \frac{MS_{Treatment}}{MS_{Error}}$$
where there are $K$ treatments and each treatment has $n_i$, $i=1,\ldots,K$ replicates.

It can be shown (in STA 463 and STA 466) that if the null hypothesis is true, $MS_{Treatment}\approx MS_{Error}$. Thus if the null hypothesis is true, $F\approx 1$. If the alternative hypothesis is true, $F>1$. 

### Performing One-Way ANOVA in R

Performing ANOVA in R is quite easy if the data has been processed correctly. The results are typically displayed in an ANOVA table but before we look at the output of the fit, we also need to check the underling assumptions for ANOVA (like we did last week with the two-sample $t$-test). First we perform the ANOVA In R:

```{r}
tire.anova <- aov(StopDist ~ tire, data=Tire)
```

That's it!  One line, using the same notation as the $t$-test, `response ~ predictor`. **We just fit a model!** We are simply telling R to model the `StopDist` response variable as a function of the predictor variable `tire`. 

When performing ANOVA we make the following assumptions.

* Underlying noise terms (residuals) are independent
* Residuals have constant variance
* Residuals are Normally distributed

As before, assessing independence must come from collection of the data (in this case, the design of the experiment). The others can be assessed graphically checking the residuals using the `autoplot()` feature in the `ggfortify` package. The residuals are essentially an estimate of the random error, or noise, terms.

```{r, warning=FALSE}
library(ggfortify)
autoplot(tire.anova)
```

* **Constant variance** - Look at "Residuals vs Fitted" or "Scale-Location" plots. The blue line should be fairly horizontal and not see any systematic patterns in the plotted points. (Put more trust in the line if the sample size is larger).
* **Normality** - Look at the "Normal Q-Q" plot. Points should reasonably match the plotted 45-degree line.

Overall, we see nothing too concerning in these plots. There could be some concern about the constant variance assumption, but there is nothing too systematic in the plot. 

### ANOVA Output

Since our assumptions check out, we can now perform statistical inference by looking at the ANOVA output.

```{r}
summary(tire.anova)
```

Here we see an $F$ = 5.328 statistic on 3 and 20 degrees of freedom, which is significantly different than the value of 1 due to the $p$-value = 0.0073. So we have evidence to suggest the different tire treads influence the stopping distance. However, this $p$-value is only valid since the underlying assumptions of ANOVA are met.

**What next?**  The *F*-test has determined that at least two tread types have different population mean stopping distances.  But, it doesn't tell us anything about *which" tread types might be different.  Do answer this, we need to do some **follow-up multiple comparisons** -- but that will wait until the next class meeting!"
